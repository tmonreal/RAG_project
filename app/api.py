"""
This file serves as the FastAPI backend where users can send questions, 
and the system will return answers generated by the LLM based on the 
relevant document context.
"""
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from embeddings import create_embeddings, get_context, check_embeddings_exist
from llm import ask_llm

app = FastAPI()

class QuestionRequest(BaseModel):
    user_name: str
    question: str
    create_embeddings: bool = False 

@app.post("/ask")
async def ask_question(request: QuestionRequest):
    # Assuming the document path is known
    document_path = 'C:\\Users\\HP\\OneDrive\\RAG_project\\data\\documento.docx'
    
    # Create document embeddings
    if request.create_embeddings:
        create_embeddings(document_path) # Create embeddings if the flag is set
    else:
        if not check_embeddings_exist():
            raise HTTPException(
                status_code=404,
                detail="Embeddings do not exist. Please create embeddings first."
            )
    
    # Get context for the question
    context = get_context(request.question)
    if not context:
        raise HTTPException(
            status_code=404,
            detail="No relevant context found for the question."
        )
    
    # Generate response from LLM
    response = ask_llm(request.question, context)
    
    return {
        "user_name": request.user_name,
        "question": request.question,
        "response": response
    }